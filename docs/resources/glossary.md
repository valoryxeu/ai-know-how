---
title: AI/ML Glossary
tags: [resources, glossary, reference]
---

# AI/ML Glossary

Quick reference for AI and machine learning terminology.

---

**Attention** — mechanism that lets models focus on relevant parts of the input when producing output. The core of transformers.

**Embeddings** — dense vector representations of data (text, images, etc.) in a continuous space where similar items are close together.

**Fine-tuning** — adapting a pre-trained model to a specific task by training it further on task-specific data.

**Hallucination** — when an AI model generates plausible-sounding but factually incorrect information.

**Inference** — using a trained model to make predictions on new data (as opposed to training).

**LoRA** — Low-Rank Adaptation. An efficient fine-tuning method that trains small adapter layers instead of the full model.

**MCP** — Model Context Protocol. An open standard for connecting AI models to external tools and data sources.

**Prompt Engineering** — the practice of crafting inputs to get desired outputs from language models.

**RAG** — Retrieval-Augmented Generation. Combining a language model with a search/retrieval system to ground responses in external data.

**RLHF** — Reinforcement Learning from Human Feedback. Training technique where human preferences guide model behavior.

**Temperature** — parameter controlling randomness in model output. Lower = more deterministic, higher = more creative.

**Token** — the basic unit of text that LLMs process. Roughly 3/4 of a word in English.

**Transformer** — the neural network architecture behind modern LLMs. Based on self-attention mechanisms.

**Vector Store** — database optimized for storing and searching embeddings (e.g., Pinecone, Weaviate, Chroma).

**Zero-shot** — using a model on a task it wasn't explicitly trained for, without any examples.

---

*This glossary grows as I learn. Each term links to deeper articles when available.*
